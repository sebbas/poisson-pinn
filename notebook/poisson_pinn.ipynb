{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXYaoQA4WaiE"
   },
   "source": [
    "# Imposing exact and soft boundary conditions in Physics-Informed NeuralÂ Networks\n",
    "\n",
    "**Author:** [sebbas](https://twitter.com/sebbas)<br>\n",
    "**Date created:** 2023/08/27<br>\n",
    "**Last modified:** 2023/09/29<br>\n",
    "**License:** MIT<br>\n",
    "**Description:** An overview of how to build PINNs with exact and soft boundary conditions by using the example of Poisson's equation with variable coefficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Z8FyC9JZttt"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this guide, you will learn how to implement a **Physics-Informed Neural Network** that can approximate instances of the **Variable Coefficient Poisson equation**. Everything will be implemented from scratch with Keras and Tensorflow!\n",
    "\n",
    "The following code includes examples on how to load training data, how to build PINN models with PDE loss and soft/exact boundary conditions, it shows how training and prediction steps work, and provides utility functions to visualize ground truth, predictions and error metrics.\n",
    "\n",
    "By the end of this tutorial, you should be able to build your own PINNs with custom BC imposition approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjjteZU-Z3SR"
   },
   "source": [
    "## Setup\n",
    "\n",
    "This guide depends on `keras` and `tensorflow`. Hence, the first step is to import these frameworks - plus the usual packages that  are commonly used when training Machine Learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Vsav5tPpN8b"
   },
   "outputs": [],
   "source": [
    "import keras.layers as KL\n",
    "import keras.regularizers as KR\n",
    "import keras.callbacks as KC\n",
    "import keras.optimizers as KO\n",
    "import keras.metrics as KM\n",
    "import keras.losses as KLS\n",
    "import keras.backend as KB\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import pandas as pd\n",
    "import scipy.interpolate as spint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, time, sys\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from IPython.display import display_markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWVOpFyLDOs4"
   },
   "source": [
    "Next, we need to gather the training data. There are two options to get training data into the working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_ltJ_Yi-p_v"
   },
   "source": [
    "### \"make clean\"\n",
    "\n",
    "Before fetching any datasets though, we make sure that the current directory is clean and no data from previous runs is present.\n",
    "\n",
    "You can remove everything that was downloaded and generated by executing the code box below. I.e. every time you need a clean slate, run the code from below in addition to restarting the run-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhskXYyx-pAf"
   },
   "outputs": [],
   "source": [
    "#%%script false --no-raise-error # uncomment this line to prevent it from being executed (e.g. to preserve models)\n",
    "!rm -rf psndata_* psnNet* *.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srH-aYGahnT9"
   },
   "source": [
    "### Option 1: Download pre-simulated datasets\n",
    "\n",
    "The code below will download datasets of pre-simulated Poisson equation problems. Each dataset contains 2D grids at different resolutions (32x32 and 128x128) for variables `a` (the variable coefficient), `f` (RHS), `g` (BC) and `p` (solution) from the Poisson equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cJQxeEVIdJt-",
    "outputId": "a9929178-50a8-402e-e095-81688b6a740b"
   },
   "outputs": [],
   "source": [
    "!rm -f psndata_*.h5\n",
    "!wget -nc https://github.com/epssmallerzero/poisson-pinn/raw/master/data/psndata_1_128_0.h5\n",
    "!wget -nc https://github.com/epssmallerzero/poisson-pinn/raw/master/data/psndata_1_128_1.h5\n",
    "!wget -nc https://github.com/epssmallerzero/poisson-pinn/raw/master/data/psndata_1_128_2.h5\n",
    "!wget -nc https://github.com/epssmallerzero/poisson-pinn/raw/master/data/psndata_1_128_3.h5\n",
    "!wget -nc https://github.com/epssmallerzero/poisson-pinn/raw/master/data/psndata_1_32_0.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntybMuvUwqSy"
   },
   "source": [
    "### Option 2: Create custom datasets\n",
    "\n",
    "You can easily generate your own Poisson equation instances using the Poisson generator from [https://github.com/sebbas/poisson-ddm#training-data-generation](https://github.com/sebbas/poisson-ddm#training-data-generation).\n",
    "\n",
    "The `README` in that repository has a section explaining how to call the generator script.\n",
    "\n",
    "Once you have a `.h5` dataset, it is best to place it alongside this Python notebook. When using Google Colab the most convenient way to access datasets is through Google Drive. Colab can mount Drive with the command from below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WG2Z1jzx_T_"
   },
   "outputs": [],
   "source": [
    "use_google_drive = 0 # toggle this when using your own datasets from Google Drive\n",
    "if use_google_drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxi6eG3ExQjh"
   },
   "source": [
    "Similarly to the pre-simulated datasets, the exact path to the datasets can be set later in the training parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOql3b89WizE"
   },
   "source": [
    "### Getting Poisson equation instances\n",
    "\n",
    "The Poisson equation datasets are available on disk now. Next, we'll need a function that can fetch individual Poisson equation instances from a dataset.\n",
    "\n",
    "To this end, we define `getBcAFP()` which returns `n` 2D Poisson equation instances of shape `shape`. Each instance contains random but smooth value distributions for variables `g`, `a`, `f`, and `p`.\n",
    "\n",
    "There is an additional option called `eqId` (equation ID). It takes values 0, 1, or 2 and determines which special case of the Poisson equation to load:\n",
    "\n",
    "- `eqId=0`: Poisson equation with zero BC\n",
    "- `eqId=1`: Laplace equation (`f=0`) with non-zero BC\n",
    "- `eqId=2`: Poisson equation with non-zero BC\n",
    "\n",
    "Throughout this tutorial, we will stick to `eqId=2`. That is, the variable coefficient Poisson equation with non-zero BC. When you experiment with this tutorial, I would encourage you to try the other equations too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLg2zF_WvdEp"
   },
   "outputs": [],
   "source": [
    "def getBcAFP(n, fname, shape, eqId):\n",
    "  dFile = h5.File(fname, 'r')\n",
    "  nSample  = dFile.attrs['nSample']\n",
    "  s        = dFile.attrs['shape']\n",
    "  assert all(x == y for x, y in zip(shape, s))\n",
    "  length   = dFile.attrs['length']\n",
    "  aData    = dFile.get('a')[:n, ...]\n",
    "  fData    = dFile.get('f')[:n, ...]\n",
    "  ppData   = dFile.get('pp')[:n, ...]\n",
    "  plData   = dFile.get('pl')[:n, ...]\n",
    "  pBcData  = dFile.get('pBc')[:n, ...]\n",
    "  dFile.close()\n",
    "\n",
    "  assert n == aData.shape[0] and n == fData.shape[0] and n == ppData.shape[0]\n",
    "\n",
    "  # Construct solution p\n",
    "  if eqId == 0: # Homogeneous Poisson\n",
    "    p = np.expand_dims(ppData, axis=-1)\n",
    "  elif eqId == 1: # Inhomogeneous Laplace\n",
    "    p = np.expand_dims(plData, axis=-1)\n",
    "    fData *= 0.0 # For now, just set f to 0 and keep in channels\n",
    "  elif eqId == 2: # Inhomogeneous Poisson (i.e. Homogeneous Poisson + Inhomogeneous Laplace)\n",
    "    pplData = ppData + plData\n",
    "    p = np.expand_dims(pplData, axis=-1)\n",
    "\n",
    "  # Construct solution, bc, a, and f arrays for Poisson / Laplace equation\n",
    "  a  = np.expand_dims(aData, axis=-1)\n",
    "  f  = np.expand_dims(fData, axis=-1)\n",
    "\n",
    "  # Construct bc array\n",
    "  bcWidth     = 1\n",
    "  # Times 2 because bc always on 2 sides in one dim\n",
    "  pad         = bcWidth * 2\n",
    "  sizeNoPad   = np.subtract(shape, pad) # Array size without padding\n",
    "  # 0s on border of ones array\n",
    "  onesWithPad = np.pad(np.ones(sizeNoPad), bcWidth)\n",
    "  # Extra dim at beginning to match batchsize and at end to match channels\n",
    "  onesExpand  = np.expand_dims(onesWithPad, axis=0)\n",
    "  onesExpand  = np.expand_dims(onesExpand, axis=-1)\n",
    "  # Repeat array 'nSample' times in 1st array dim\n",
    "  bc          = np.repeat(onesExpand, n, axis=0)\n",
    "  # Fill boundary with Dirichlet bc values (data from Laplace solve, ie f=0)\n",
    "  if eqId == 1 or eqId == 2: # Inhomogeneous Poisson / Laplace\n",
    "    nx, ny = shape\n",
    "    bc[:,  0,  :, 0] = p[:,  0,  :, 0] # i- boundary\n",
    "    bc[:,  :, -1, 0] = p[:,  :, -1, 0] # j+ boundary\n",
    "    bc[:, -1,  :, 0] = p[:, -1,  :, 0] # i+ boundary\n",
    "    bc[:,  :,  0, 0] = p[:,  :,  0, 0] # j- boundary\n",
    "    # Average bc values to counterbalance overlap in corner cells\n",
    "    if 0:\n",
    "      bcCnt = np.ones_like(bc)\n",
    "      corners = [[0,0], [nx-1,0], [0,ny-1], [nx-1,ny-1]]\n",
    "      for x,y in corners:\n",
    "        bcCnt[:,x,y,0] += 1\n",
    "      bc /= bcCnt\n",
    "\n",
    "  # Combine bc, a, f along channel dim\n",
    "  bcAF = np.concatenate((bc, a, f), axis=-1)\n",
    "\n",
    "  return bcAF, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoTR8B-SZTKi"
   },
   "source": [
    "### Plots and metrics\n",
    "\n",
    "Lastly, before getting into the actual model setup, we define several utility functions. These will come in handy when evaluating the accuracy of our models. The metric functions will probably look familiar to you. The plotting functions make use of them and will be called once we have trained our first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZNM3W81pAf1"
   },
   "outputs": [],
   "source": [
    "def rmse(targets, predictions):\n",
    "  return np.sqrt(np.mean((targets-predictions)**2))\n",
    "\n",
    "\n",
    "def mae(targets, predictions):\n",
    "  return np.mean(np.abs(targets-predictions))\n",
    "\n",
    "\n",
    "def mape(targets, predictions):\n",
    "  return np.mean(np.abs((targets - predictions) / targets)) * 100.0\n",
    "\n",
    "\n",
    "def getFileName(type, prefix, eqId, archId, bcId):\n",
    "  return f'{prefix}_{type}_eq-{eqId}_arch-{archId}_bc-{bcId:d}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3TvLkGbo4s-"
   },
   "outputs": [],
   "source": [
    "def plotSolution(sP, nPred, p, phat, prefix, eqId, archId, bcId):\n",
    "  sns.set_style(\"ticks\")\n",
    "  fig = plt.figure(figsize=(10, 2), dpi=300, constrained_layout=True)\n",
    "\n",
    "  eP = sP + nPred\n",
    "  errorP = np.abs(p[sP:eP,:,:,0] - phat[0:nPred,:,:,0])\n",
    "\n",
    "  # Min, max values, needed for colobar range\n",
    "  minP, maxP       = np.min(p[sP:eP,:,:,0]), np.max(p[sP:eP,:,:,0])\n",
    "  minPErr, maxPErr = np.min(errorP[0:nPred,:,:]), np.max(errorP[0:nPred,:,:])\n",
    "  xTicks, yTicks   = [0, p.shape[1]-1], [0, p.shape[2]-1]\n",
    "\n",
    "  nRows = 1\n",
    "  nCols = 4\n",
    "  iInput = sP\n",
    "\n",
    "  ax = fig.add_subplot(nPred, nCols, 1)\n",
    "  plt.title('p')\n",
    "  plt.imshow(p[iInput,:,:,0], vmin=-1, vmax=1, origin='lower', cmap='jet')\n",
    "  plt.colorbar()\n",
    "  plt.xticks(xTicks)\n",
    "  plt.yticks(yTicks)\n",
    "\n",
    "  ax = fig.add_subplot(nPred, nCols, 2)\n",
    "  plt.title('phat')\n",
    "  plt.imshow(phat[0,:,:,0], vmin=-1, vmax=1, origin='lower', cmap='jet')\n",
    "  plt.colorbar()\n",
    "  plt.xticks(xTicks)\n",
    "  plt.yticks(yTicks)\n",
    "\n",
    "  ax = fig.add_subplot(nPred, nCols, 3)\n",
    "  plt.title('abs(p-phat)')\n",
    "  plt.imshow(errorP[0,:,:], vmin=0, vmax=0.1, origin='lower', cmap='jet')\n",
    "  plt.colorbar()\n",
    "  plt.xticks(xTicks)\n",
    "  plt.yticks(yTicks)\n",
    "\n",
    "  ax = fig.add_subplot(nPred, nCols, 4)\n",
    "  ax.axis('off')\n",
    "  plt.title('Stats')\n",
    "  plt.text(0.2, 0.7, 'MAPE: %.4f %%' % mape(p[sP,:,:,0], phat[0,:,:,0]))\n",
    "  plt.text(0.2, 0.5, 'RMSE: %.4f' % rmse(p[sP,:,:,0], phat[0,:,:,0]))\n",
    "  plt.text(0.2, 0.3, 'MAE: %.4f' % mae(p[sP,:,:,0], phat[0,:,:,0]))\n",
    "\n",
    "  fname = getFileName('solutions', prefix, eqId, archId, bcId)\n",
    "  plt.savefig(f'{fname}.png', bbox_inches='tight')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNISVS41Mq-4"
   },
   "outputs": [],
   "source": [
    "def plotMetrics(model, epochStart, epochEnd, name, eqId, archId, bcId, plotLr=False):\n",
    "  history = pd.read_csv(model + '.log', sep = ',', engine='python')[epochStart:epochEnd]\n",
    "\n",
    "  titles  = ['Overall loss', 'MAE', 'Data loss', 'PDE loss']\n",
    "  metrics = [['loss', 'val_loss'], ['mae', 'val_mae'], \\\n",
    "             ['data', 'val_data'], ['pde', 'val_pde']]\n",
    "  nCols, nRows = 2, 2\n",
    "\n",
    "  sns.set_style(\"darkgrid\")\n",
    "  fig = plt.figure(figsize=(10, 5), dpi=300)\n",
    "  fig.subplots_adjust(hspace=.5)\n",
    "\n",
    "  for i, names in enumerate(metrics):\n",
    "    ax = fig.add_subplot(nRows, nCols, i+1)\n",
    "    ax.set_yscale('log')\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: int(x)))\n",
    "    for j, n in enumerate(names):\n",
    "      assert n in names\n",
    "      sns.lineplot(data=history, x=history['epoch'], y=n, legend='brief', label=n)\n",
    "\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    ax.title.set_text(titles[i])\n",
    "    if plotLr:\n",
    "      sns.lineplot(history['lr'], color='C3')\n",
    "\n",
    "  fname = getFileName('losses', name, eqId, archId, bcId)\n",
    "  plt.savefig(f'{fname}.png', bbox_inches='tight')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1xpv26hZjs7"
   },
   "outputs": [],
   "source": [
    "def plotMetricsTable(p, phatSoft, phatExact, sTime, eTime):\n",
    "  sMAPE = mape(p[0,:,:,0], phatSoft[0,:,:,0])\n",
    "  sRMSE = rmse(p[0,:,:,0], phatSoft[0,:,:,0])\n",
    "  sMAE  = mae(p[0,:,:,0], phatSoft[0,:,:,0])\n",
    "\n",
    "  eMAPE = mape(p[0,:,:,0], phatExact[0,:,:,0])\n",
    "  eRMSE = rmse(p[0,:,:,0], phatExact[0,:,:,0])\n",
    "  eMAE  = mae(p[0,:,:,0], phatExact[0,:,:,0])\n",
    "\n",
    "  return f'''\n",
    "  | Model      |  MAE       | RMSE        | MAPE         | Training time (sec) |\n",
    "  | :--------: | :--------: | :---------: | :----------: | :-----------------: |\n",
    "  | Soft BCs   | {sMAE:.2e} | {sRMSE:.2e} | {sMAPE:.2f}% | {sTime:.2f}         |\n",
    "  | Exact BCs  | {eMAE:.2e} | {eRMSE:.2e} | {eMAPE:.2f}% | {eTime:.2f}         |\n",
    "  '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXeyZp9gXqdG"
   },
   "source": [
    "## Generators\n",
    "\n",
    "We need to have ways to generate points uniformly/randomly and exactly at grid indices. Therefore, we define a continuous and a discrete point generator.\n",
    "\n",
    "The former will be used to generate points during training. The latter will serve during the prediction step where points coinciding with pixel positions will be needed so that model outputs can easily be compared to the ground truth and plotted as images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZxAIJiTlId8"
   },
   "source": [
    "### The continuous point generator\n",
    "\n",
    "The continuous generator can sample points uniformly and randomly in a given range [`minCoord`, `maxCoord`] - the size of our domain. While point coordinates are random, the location of points can be finetuned by distinguishing between collocation and boundary points: Collocation points can end up anywhere in the domain, boundary points will always be sampled near domain boundaries.\n",
    "\n",
    "The number of points for each point type can be specified with the `nColPts` and `nBndPts` arguments. A good rule of thumb is to use a 10:1 ratio of collocation and boundary points. This ensures, that\n",
    "\n",
    "1. there will always be a sufficient amount of points inside the domain (ensures model accuracy), and\n",
    "2. the density of points along domain boundaries is higher relative to the domain interior (ensures that the model learns the BC found at domain boundaries).\n",
    "\n",
    "We chose this setup as PINNs tend to converge faster when the BC can be learned accurately. However, to optimize model convergence further, it is best to try out multiple different point ratios.\n",
    "\n",
    "### Interpolating from ground truth data\n",
    "\n",
    "You might wonder how we can generate points at random positions and with training data for `a`, `f`, and `p` when the ground truth data is only available on a discrete grid (i.e. the simulated Poisson data we obtain from the `.h5` datasets).\n",
    "\n",
    "The continuous generator goes around this problem by interpolating points. While this operation carries an error it is still better than only using the data found at grid indices. Especially when domains are smaller (e.g. 32x32 grids), interpolation augments the possible number of training points and increases model accuracy.\n",
    "\n",
    "When experimenting with the generators I would recommend searching for `RegularGridInterpolator` in the generator below. The interpolation method, for example, could also be set to `cubic` for increased precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nir8JH5yzOzs"
   },
   "outputs": [],
   "source": [
    "def generatePtsCont(begin, end, input, label, batchsize=32, loop=True, shuffle=True, \\\n",
    "                    minCoord=0, maxCoord=1, exactBc=False, nColPts=1e4, nBndPts=0, nBcPts=0):\n",
    "  assert label.shape == input[:,:,:,0:1].shape\n",
    "  size = (label.shape[1], label.shape[2])\n",
    "\n",
    "  # Transform grid indices to value range from args\n",
    "  xx = np.linspace(minCoord, maxCoord, num=size[0])\n",
    "  yy = np.linspace(minCoord, maxCoord, num=size[1])\n",
    "  xyBcBatch, pBcBatch = None, None\n",
    "\n",
    "  # spint.RegularGridInterpolator expects ij indexing\n",
    "  bc = np.expand_dims(np.swapaxes(input[0,:,:,0], 0, 1), axis=-1)\n",
    "  a = np.expand_dims(np.swapaxes(input[0,:,:,1], 0, 1), axis=-1)\n",
    "  f = np.expand_dims(np.swapaxes(input[0,:,:,2], 0, 1), axis=-1)\n",
    "  p = np.expand_dims(np.swapaxes(label[0,:,:,0], 0, 1), axis=-1)\n",
    "  # Interpolation functions for discrete grid data\n",
    "  interpMethod = 'linear' # or use 'nearest' or 'cubic' - slower but more precise\n",
    "  bcInterp = spint.RegularGridInterpolator((xx, yy), bc, method=interpMethod)\n",
    "  aInterp = spint.RegularGridInterpolator((xx, yy), a, method=interpMethod)\n",
    "  fInterp = spint.RegularGridInterpolator((xx, yy), f, method=interpMethod)\n",
    "  pInterp = spint.RegularGridInterpolator((xx, yy), p, method=interpMethod)\n",
    "\n",
    "  # Generate random x, y values in range\n",
    "  eps = 0. # optional space to domain boundary\n",
    "  x = np.random.uniform(minCoord+eps, maxCoord-eps, size=nColPts)\n",
    "  y = np.random.uniform(minCoord+eps, maxCoord-eps, size=nColPts)\n",
    "  xy = np.concatenate((x[:, np.newaxis], y[:, np.newaxis]), axis=-1)\n",
    "\n",
    "  # Generate additional points along domain boundaries\n",
    "  if nBndPts > 0:\n",
    "    xyBnd = _generatePtsBnd(nBndPts, random=True)\n",
    "    xy = np.concatenate((xy, xyBnd), axis=0) # Add boundary points to global point array\n",
    "\n",
    "  if shuffle:\n",
    "    perm = np.random.permutation(xy.shape[0])\n",
    "    xy = xy[perm]\n",
    "\n",
    "  if exactBc and nBcPts > 0:\n",
    "    xyBc = _generatePtsBnd(nBcPts, random=False)\n",
    "    pBc = bcInterp(xyBc) # Solution at bnd points\n",
    "    # Repeat bnd xy and p batch size times (every point in batch has a copy of bnd points)\n",
    "    xyBcBatch = np.expand_dims(xyBc, axis=0)\n",
    "    xyBcBatch = np.repeat(xyBcBatch, batchsize, axis=0)\n",
    "    pBcBatch = np.expand_dims(pBc, axis=0)\n",
    "    pBcBatch = np.repeat(pBcBatch, batchsize, axis=0)\n",
    "\n",
    "  # Infinite generator loop, abort only after covering all pts and when loop arg is false\n",
    "  abort = False\n",
    "  s = begin\n",
    "  while not abort:\n",
    "    if loop and s + batchsize > end:\n",
    "      s = 0\n",
    "    e = s + batchsize\n",
    "    if e >= end:\n",
    "      e = end\n",
    "      abort = True\n",
    "    yield [xy[s:e,:], pInterp(xy[s:e,:]), xyBcBatch, pBcBatch, aInterp(xy[s:e,:]), bcInterp(xy[s:e,:]), fInterp(xy[s:e,:])], []\n",
    "    s += batchsize\n",
    "\n",
    "\n",
    "def _generatePtsBnd(nPts, random=False, minCoord=0, maxCoord=1, eps=0):\n",
    "  assert nPts > 0, 'Must supply additional boundary points when using exact boundary condition'\n",
    "\n",
    "  if random:\n",
    "    xLow = np.random.uniform(minCoord+eps, maxCoord-eps, size=nPts) # bottom\n",
    "    xUp  = np.random.uniform(minCoord+eps, maxCoord-eps, size=nPts) # top\n",
    "    yLow = np.random.uniform(minCoord+eps, maxCoord-eps, size=nPts) # left\n",
    "    yUp  = np.random.uniform(minCoord+eps, maxCoord-eps, size=nPts) # right\n",
    "  else:\n",
    "    offset = 1 / nPts\n",
    "    xLow = np.linspace(minCoord+eps, maxCoord-eps-offset, num=nPts)\n",
    "    xUp  = np.linspace(minCoord+eps + offset, maxCoord-eps, num=nPts)\n",
    "    yLow = np.linspace(minCoord+eps + offset, maxCoord-eps, num=nPts)\n",
    "    yUp  = np.linspace(minCoord+eps, maxCoord-eps-offset, num=nPts)\n",
    "\n",
    "  mins  = np.repeat(minCoord+eps, nPts)\n",
    "  maxs  = np.repeat(maxCoord-eps, nPts)\n",
    "\n",
    "  # Append bnd points to existing xy\n",
    "  xy = np.empty((0,2))\n",
    "  for a, b in list(zip([xLow, xUp, mins, maxs], [mins, maxs, yLow, yUp])):\n",
    "    tmp = np.concatenate((a[:, np.newaxis], b[:, np.newaxis]), axis=-1)\n",
    "    xy = np.concatenate((xy, tmp), axis=0)\n",
    "\n",
    "  return xy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xd9bgRegqpMv"
   },
   "source": [
    "### Exact BC option\n",
    "\n",
    "The `exactBc` option will only be used when generating points for exact BC models. When using it, the generator will uniformly sample `nBcPts` additional  points along domain boundaries. These points become the BC vector which is repeated for every point in a batch (\"the BC is broadcasted to every point in the generator batch\"). When the generator yields a batch, the BC vector is part of the output that the model receives.\n",
    "\n",
    "This way, every training point in an exact BC model knows what the BC looks like. Later on, when defining the exact BC model, we will see how models process the BC (hint: the BC is not part of the model input)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8J1sAw7DZJ6A"
   },
   "source": [
    "\n",
    "### The discrete point generator\n",
    "\n",
    "For every grid index in the `input` data, the discrete generator should generate the `xy` coordinates and corresponding `a`, `f`, and `p` Poisson values found at grid indices in a domain with given `minCoord` and `maxCoord`.\n",
    "\n",
    "The idea is to use this generator during the prediction stage. This way, predictions can be compared to the ground truth and error metrics such as MSE can be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNW-OYC0Xnqh"
   },
   "outputs": [],
   "source": [
    "def generatePtsDisc(begin, end, input, label, batchsize=1024, loop=True, shuffle=True, \\\n",
    "                    minCoord=0, maxCoord=1, exactBc=False):\n",
    "  assert label.shape == input[:,:,:,0:1].shape\n",
    "  size = (label.shape[1], label.shape[2])\n",
    "\n",
    "  # Transform grid indices to value range from args\n",
    "  xx = np.linspace(minCoord, maxCoord, num=size[0])\n",
    "  yy = np.linspace(minCoord, maxCoord, num=size[1])\n",
    "  xyBcBatch, pBcBatch = None, None\n",
    "\n",
    "  bcFlat = input[:,:,:,0].flatten()\n",
    "  aFlat = input[:,:,:,1].flatten()\n",
    "  fFlat = input[:,:,:,2].flatten()\n",
    "  pFlat = label.flatten()\n",
    "  if shuffle:\n",
    "    perm = np.random.permutation(len(bcFlat))\n",
    "    bcFlat = bcFlat[perm]\n",
    "    aFlat = aFlat[perm]\n",
    "    fFlat = fFlat[perm]\n",
    "    pFlat = pFlat[perm]\n",
    "  bcFlat = np.expand_dims(bcFlat, axis=-1)\n",
    "  aFlat = np.expand_dims(aFlat, axis=-1)\n",
    "  fFlat = np.expand_dims(fFlat, axis=-1)\n",
    "  pFlat = np.expand_dims(pFlat, axis=-1)\n",
    "\n",
    "  x, y = np.meshgrid(xx, yy, indexing='xy')\n",
    "  x, y = x.flatten(), y.flatten()\n",
    "  if shuffle:\n",
    "    x = x[perm]\n",
    "    y = y[perm]\n",
    "  x = np.expand_dims(x, axis=-1)\n",
    "  y = np.expand_dims(y, axis=-1)\n",
    "  xy = np.concatenate((x, y), axis=-1)\n",
    "\n",
    "  if exactBc:\n",
    "    p = label[0,:,:,0]\n",
    "    pLst = [p[0,:-1], p[:-1,-1], p[-1,1:], p[1:,0]] # left, top, right, bottom\n",
    "    xi, yj = np.meshgrid(xx, yy, indexing='xy')\n",
    "    xLst = [xi[0,:-1], xi[:-1,-1], xi[-1,1:], xi[1:,0]] # left, top, right, bottom\n",
    "    yLst = [yj[0,:-1], yj[:-1,-1], yj[-1,1:], yj[1:,0]] # left, top, right, bottom\n",
    "\n",
    "    xyBc = np.empty((0,2))\n",
    "    pBc = np.empty((0,1))\n",
    "    for arrX, arrY, arrP in list(zip(xLst, yLst, pLst)):\n",
    "      arrX, arrY, arrP =  arrX[:, np.newaxis], arrY[:, np.newaxis], arrP[:, np.newaxis]\n",
    "      tmp = np.concatenate((arrX, arrY), axis=-1)\n",
    "      xyBc = np.concatenate((xyBc, tmp), axis=0)\n",
    "      pBc = np.concatenate((pBc, arrP), axis=0)\n",
    "\n",
    "    # Repeat bnd xy and p batch size times (every point in batch has a copy of bnd points)\n",
    "    xyBcBatch = np.expand_dims(xyBc, axis=0)\n",
    "    xyBcBatch = np.repeat(xyBcBatch, batchsize, axis=0)\n",
    "    pBcBatch = np.expand_dims(pBc, axis=0)\n",
    "    pBcBatch = np.repeat(pBcBatch, batchsize, axis=0)\n",
    "\n",
    "  # Infinite generator loop, abort only after covering all pts and when loop arg is false\n",
    "  abort = False\n",
    "  s = begin\n",
    "  while not abort:\n",
    "    if loop and s + batchsize > end:\n",
    "      s = 0\n",
    "    e = s + batchsize\n",
    "    if e >= end:\n",
    "      e = end\n",
    "      abort = True\n",
    "    yield [xy[s:e,:], pFlat[s:e,:], xyBcBatch, pBcBatch, aFlat[s:e,:], bcFlat[s:e,:], fFlat[s:e,:]], []\n",
    "    s += batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJG2UQeiU-ux"
   },
   "source": [
    "### Verifying generators\n",
    "\n",
    "We can write a simple plotting function to visualize the points from the generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HOSjEDhTm8w"
   },
   "outputs": [],
   "source": [
    "def plotPoints(nPts, fname, bcAF, p, discrete=False):\n",
    "  if discrete:\n",
    "    ptsGen = generatePtsDisc(0, nPts, bcAF, p, batchsize=nPts, loop=False)\n",
    "    name = 'discrete'\n",
    "  else:\n",
    "    ptsGen = generatePtsCont(0, nPts, bcAF, p, batchsize=nPts, nColPts=nPts, loop=False)\n",
    "    name = 'continuous'\n",
    "\n",
    "  xy = next(ptsGen)[0][0]\n",
    "\n",
    "  sns.set_style('darkgrid')\n",
    "  fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "  ax.set_title(f'{nPts} points from {name} point generator')\n",
    "  ax.set_box_aspect(1)\n",
    "\n",
    "  sns.scatterplot(x=xy[:,0], y=xy[:,1], alpha=0.3, edgecolor=\"black\", marker=\"o\", ax=ax)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "  fig.savefig(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaaErqdAWDqj"
   },
   "source": [
    "For illustration purposes, we plot 400 points: Once with the continuous point generator ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "BDforVg_Uos2",
    "outputId": "f9bbd0c0-ef30-42e7-a2db-e5fb27515c85"
   },
   "outputs": [],
   "source": [
    "gs = 32\n",
    "bcAF, p  = getBcAFP(n=1, fname=f'psndata_1_{gs}_0.h5', shape=(gs, gs), eqId=2)\n",
    "plotPoints(400, 'continuous_points.png', bcAF, p, discrete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw9Ju9LVY2Ya"
   },
   "source": [
    "... and once with the discrete point generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "75PyUbrCZBpv",
    "outputId": "56309806-775c-4b01-e179-9702c0f18764"
   },
   "outputs": [],
   "source": [
    "plotPoints(400, 'discrete_points.png', bcAF, p, discrete=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fd4Vk5WYQnl"
   },
   "source": [
    "## PINN with Soft BCs\n",
    "\n",
    "We finally get to the implementation of the first PINN! That is, the PINN for soft BCs. The model is built on top of `keras.Model` and uses custom `train_step` and `test_step` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdmR0N0jDf2h"
   },
   "source": [
    "### Model setup\n",
    "\n",
    "Here are some interesting observations about the model:\n",
    "- The architecture goes in as a list of strings (`operators` argument) which the model converts to `keras` layers and stores them in `self.mlp`\n",
    "- `call()` uses the layers from `self.mlp` and concatenates them in `keras` functional API fashion.\n",
    "  - This kind of model setup makes it possible to quickly try out various architectures. All it takes is a new `architecture` string in the training parameters.\n",
    "- `_compute_losses()` contains the functionality for automatic differentiation.\n",
    "  - Note that we need to differentiate twice to solve the Poisson equation, and hence, there are two nested `GradientTape`'s.\n",
    "  - When solving other PDEs, this setup should change accordingly. I.e. if the PDE has order 1, we obviously don't have to differentiate twice.\n",
    "  - Depending on the distance to the domain boundaries, we compute a PDE or data loss for every point.\n",
    "  - The data loss is where the model learns the soft BC!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qK0OIVxyXDGl"
   },
   "outputs": [],
   "source": [
    "class PsnPinn(tf.keras.Model):\n",
    "  def __init__(self, operators=[], reg=None, alpha=0.01, save_grad_stat=False, **kwargs):\n",
    "    super(PsnPinn, self).__init__(**kwargs)\n",
    "    assert len(operators) > 0\n",
    "    if reg is not None: assert len(reg) == len(operators)\n",
    "    self.alpha        = alpha\n",
    "    self.operators    = operators\n",
    "    self.reg          = np.zeros(len(operators)) if reg==None else np.array(reg)\n",
    "    self.saveGradStat = save_grad_stat\n",
    "\n",
    "    tf.print('PsnPinn with architecture:')\n",
    "\n",
    "    # Input parameters will be filled with info from input layer (1st in arch string)\n",
    "    self.inputLayer = None\n",
    "    self.inputShape = None\n",
    "    self.batchSize  = None\n",
    "\n",
    "    self.mlp = []\n",
    "    for i, op in enumerate(self.operators):\n",
    "\n",
    "      layerName, layerArgs = self._getLayerName(op)\n",
    "\n",
    "      ## 1st layer must be an input layer\n",
    "      if layerName == 'input':\n",
    "        self.inputShape = (layerArgs[0],)\n",
    "        self.batchSize = layerArgs[1]\n",
    "        self.inputLayer = ( KL.InputLayer( input_shape=self.inputShape,\n",
    "                                           batch_size=self.batchSize) )\n",
    "        tf.print(f'==> {layerName} layer with input shape {self.inputShape}, batch size {self.batchSize}')\n",
    "        continue\n",
    "\n",
    "      assert i > 0, 'Invalid architecture, missing input layer'\n",
    "\n",
    "      if layerName == 'batchnorm':\n",
    "        self.mlp.append( KL.BatchNormalization() )\n",
    "\n",
    "      elif layerName == 'leakyrelu':\n",
    "        self.mlp.append( KL.LeakyReLU() )\n",
    "\n",
    "      elif layerName == 'relu':\n",
    "        self.mlp.append( KL.ReLU() )\n",
    "\n",
    "      elif layerName == 'concat':\n",
    "        self.mlp.append( KL.Concatenate() )\n",
    "\n",
    "      elif layerName == 'dense':\n",
    "        if layerArgs[1] == 0:\n",
    "          activation = 'gelu'\n",
    "        elif layerArgs[1] == 1:\n",
    "          activation = 'linear'\n",
    "        else:\n",
    "          print('Invalid activation in Dense layer')\n",
    "          sys.exit()\n",
    "        self.mlp.append( KL.Dense( units=layerArgs[0], activation=activation) )\n",
    "\n",
    "      tf.print(f'==> {layerName} layer with args {layerArgs[0:]}')\n",
    "\n",
    "    # Dicts for metrics and statistics\n",
    "    self.trainMetrics = {}\n",
    "    self.validMetrics = {}\n",
    "    # Construct metric names and add to train/valid dicts\n",
    "    names = ['loss', 'data', 'pde']\n",
    "    for key in names:\n",
    "      self.trainMetrics[key] = KM.Mean(name='train_'+key)\n",
    "      self.validMetrics[key] = KM.Mean(name='valid_'+key)\n",
    "    self.trainMetrics['mae'] = KM.MeanAbsoluteError(name='train_mae')\n",
    "    self.validMetrics['mae'] = KM.MeanAbsoluteError(name='valid_mae')\n",
    "\n",
    "    ## Add metrics for layers' weights, if save_grad_stat is required\n",
    "    ## i even for weights, odd for bias\n",
    "    if self.saveGradStat:\n",
    "      for i, op in enumerate(self.operators):\n",
    "        if op.trainable:\n",
    "          names = ['dat_'+repr(i)+'w_avg', 'dat_'+repr(i)+'w_std',\\\n",
    "                   'dat_'+repr(i)+'b_avg', 'dat_'+repr(i)+'b_std',\\\n",
    "                   'pde_'+repr(i)+'w_avg', 'pde_'+repr(i)+'w_std',\\\n",
    "                   'pde_'+repr(i)+'b_avg', 'pde_'+repr(i)+'b_std']\n",
    "          for name in names:\n",
    "            self.trainMetrics[name] = KM.Mean(name='train '+name)\n",
    "\n",
    "    # Dicts to save training and validation statistics\n",
    "    self.trainStat = {}\n",
    "    self.validStat = {}\n",
    "\n",
    "\n",
    "  def _getLayerName(self, operator):\n",
    "    strs = operator.split('_')\n",
    "    layerName = strs[0]\n",
    "    layerArgs = [int(s) for s in strs[1:]] # Convert layer args from string to int\n",
    "    return layerName, layerArgs\n",
    "\n",
    "\n",
    "  def _isBc(self, xy, eps=1e-2, minDomain=[0,0], maxDomain=[1,1]):\n",
    "    xLowerBc = tf.less_equal(    xy[:,0], minDomain[0] + eps )\n",
    "    xUpperBc = tf.math.greater_equal( xy[:,0], maxDomain[0] - eps )\n",
    "    yLowerBc = tf.math.less_equal(    xy[:,1], minDomain[0] + eps )\n",
    "    yUpperBc = tf.math.greater_equal( xy[:,1], maxDomain[0] - eps )\n",
    "\n",
    "    isBcX = tf.math.logical_or(xLowerBc, xUpperBc)\n",
    "    isBcY = tf.math.logical_or(yLowerBc, yUpperBc)\n",
    "    isBc = tf.cast(tf.math.logical_or(isBcX, isBcY), tf.float32)\n",
    "    return isBc\n",
    "\n",
    "\n",
    "  def call(self, inputs, training=False, withInputLayer=1):\n",
    "    xy = inputs[0]\n",
    "    tensors = [self.inputLayer(xy)] if withInputLayer else [xy]\n",
    "\n",
    "    for i, (curLayer, op) in enumerate(zip(self.mlp, self.operators[0:])):\n",
    "      layerName, layerArgs = self._getLayerName(op)\n",
    "      if layerName == 'concat':\n",
    "        p1, p2 = layerArgs[0], layerArgs[1] # indices in operator list of tensors to concat\n",
    "        resultTensor = curLayer([tensors[p1], tensors[p2]])\n",
    "      else:\n",
    "        curTensor = tensors[-1]\n",
    "        if layerName == 'batchnorm':\n",
    "          resultTensor = curLayer(curTensor, training=training)\n",
    "        else:\n",
    "          resultTensor = curLayer(curTensor)\n",
    "      tensors.append(resultTensor)\n",
    "\n",
    "    return resultTensor\n",
    "\n",
    "\n",
    "  def _compute_losses(self, xy, a, f, p, xyBc, pBc):\n",
    "    with tf.GradientTape(watch_accessed_variables=False, persistent=True) as tape2:\n",
    "      tape2.watch(xy)\n",
    "      with tf.GradientTape(watch_accessed_variables=False, persistent=True) as tape1:\n",
    "        tape1.watch(xy)\n",
    "        pPred = self([xy, p, xyBc, pBc], training=True)\n",
    "      # 1st order derivatives\n",
    "      p_grad = tape1.gradient(pPred, xy)\n",
    "      p_x, p_y = p_grad[:,0], p_grad[:,1]\n",
    "      del tape1\n",
    "    # 2nd order derivatives\n",
    "    p_xx = tape2.gradient(p_x, xy)[:,0]\n",
    "    p_yy = tape2.gradient(p_y, xy)[:,1]\n",
    "    del tape2\n",
    "\n",
    "    # Convert to row vector\n",
    "    a = tf.transpose(a)[0]\n",
    "    f = tf.transpose(f)[0]\n",
    "\n",
    "    # Find points close to domain boundary\n",
    "    isBc  = self._isBc(xy, eps=0.2)\n",
    "    isCol = tf.abs(1.0 - isBc)\n",
    "\n",
    "    # Compute PDE loss\n",
    "    pde = a * (p_xx + p_yy)\n",
    "    pdeLoss = KLS.mean_squared_error(f*isCol, pde*isCol)\n",
    "\n",
    "    # Convert to row vector\n",
    "    p = tf.transpose(p)[0]\n",
    "    pPred = tf.transpose(pPred)[0]\n",
    "    # Compute data loss\n",
    "    dataLoss = KLS.mean_squared_error(p*isBc, pPred*isBc)\n",
    "\n",
    "    return dataLoss, pdeLoss, p, pPred\n",
    "\n",
    "\n",
    "  def train_step(self, data):\n",
    "    xy   = data[0][0]\n",
    "    p    = data[0][1]\n",
    "    xyBc = data[0][2]\n",
    "    pBc  = data[0][3]\n",
    "    a    = data[0][4]\n",
    "    bc   = data[0][5]\n",
    "    f    = data[0][6]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      dataLoss, pdeLoss, p, pPred = self._compute_losses(xy, a, f, p, xyBc, pBc)\n",
    "      loss = dataLoss + self.alpha * pdeLoss\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, self.trainable_variables)\n",
    "    # Update weights\n",
    "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "    # Update metrics\n",
    "    self.trainMetrics['loss'].update_state(loss)\n",
    "    self.trainMetrics['data'].update_state(dataLoss)\n",
    "    self.trainMetrics['pde'].update_state(pdeLoss)\n",
    "    self.trainMetrics['mae'].update_state(p, pPred)\n",
    "\n",
    "    # Return metrics in statistics dictionary\n",
    "    for key in self.trainMetrics:\n",
    "      self.trainStat[key] = self.trainMetrics[key].result()\n",
    "    return self.trainStat\n",
    "\n",
    "\n",
    "  def test_step(self, data):\n",
    "    xy   = data[0][0]\n",
    "    p    = data[0][1]\n",
    "    xyBc = data[0][2]\n",
    "    pBc  = data[0][3]\n",
    "    a    = data[0][4]\n",
    "    bc   = data[0][5]\n",
    "    f    = data[0][6]\n",
    "\n",
    "    dataLoss, pdeLoss, p, pPred = self._compute_losses(xy, a, f, p, xyBc, pBc)\n",
    "    loss = dataLoss + self.alpha * pdeLoss\n",
    "\n",
    "    # Update metrics\n",
    "    self.validMetrics['loss'].update_state(loss)\n",
    "    self.validMetrics['data'].update_state(dataLoss)\n",
    "    self.validMetrics['pde'].update_state(pdeLoss)\n",
    "    self.validMetrics['mae'].update_state(p, pPred)\n",
    "\n",
    "    # Return metrics in statistics dictionary\n",
    "    for key in self.trainMetrics:\n",
    "      self.validStat[key] = self.validMetrics[key].result()\n",
    "    return self.validStat\n",
    "\n",
    "\n",
    "  def reset_metrics(self):\n",
    "    for key in self.trainMetrics:\n",
    "      self.trainMetrics[key].reset_states()\n",
    "    for key in self.validMetrics:\n",
    "      self.validMetrics[key].reset_states()\n",
    "\n",
    "\n",
    "  @property\n",
    "  def metrics(self):\n",
    "    return [self.trainMetrics[key] for key in self.trainMetrics] \\\n",
    "         + [self.validMetrics[key] for key in self.validMetrics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qg-PoGkTjy1F"
   },
   "source": [
    "### Parameters\n",
    "\n",
    "The following parameters will be used in both the soft and the exact BC models. We define them here once.\n",
    "\n",
    "Here are some ideas to try when playing with the model:\n",
    "- Define a new architecture by adding its string representation and `archId`.\n",
    "- Increase `alpha`, the PDE loss contribution.\n",
    "- Load a different dataset by adjusting `datasetId`.\n",
    "- Train a different variation of the Poisson equation, e.g. `eqId=0` for zero-BC Poisson.\n",
    "- Train for more epochs and adjust the learning rate along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dApKLyAnb_Kf"
   },
   "outputs": [],
   "source": [
    "# Variables (feel free to adjust these!)\n",
    "prefix      = 'psnNet'\n",
    "archId      = 1\n",
    "eqId        = 2\n",
    "batchsize   = 64\n",
    "reg         = None\n",
    "patience    = 200\n",
    "initEpoch   = 0\n",
    "lr0         = 5e-4       # Initial learning rate\n",
    "lrmin       = 1e-7       # Minimum learning rate (used in callbacks)\n",
    "lrRestart   = 5e-5       # Learning rate to use when reloading the mode\n",
    "alpha       = 0.1        # Hyperparameter for PDE loss contribution. Based on int(nBndPts / nColPts)\n",
    "shape       = (128, 128) # Grid size from dataset\n",
    "datasetId   = 2          # Choose one of datasets 0, 1, 2, 3\n",
    "fname       = f'psndata_1_{shape[0]}_{datasetId}.h5'\n",
    "initEpoch   = 0\n",
    "if use_google_drive:\n",
    "  fname     =  '/content/drive/MyDrive/Colab Notebooks/' + fname\n",
    "if archId == 1: # Fully-connected MLP\n",
    "  architecture = ['input_{}_{}',\n",
    "                  'dense_128_0',\n",
    "                  'dense_128_0',\n",
    "                  'dense_128_0',\n",
    "                  'dense_128_0',\n",
    "                  'dense_1_1']\n",
    "elif archId == 2:        # Fully-connected (cone-shape) MLP\n",
    "  architecture = ['input_{}_{}',\n",
    "                  'dense_128_0',\n",
    "                  'dense_64_0',\n",
    "                  'dense_32_0',\n",
    "                  'dense_16_0',\n",
    "                  'dense_1_1']\n",
    "\n",
    "# Constants (do not change these!)\n",
    "nGridPts    = shape[0] * shape[1]\n",
    "nSample     = 1\n",
    "nPinnInputs = 2          # Model inputs are x and y -> nPinnInputs = 2\n",
    "eagerExec   = False      # Only enable when debugging\n",
    "# Insert values variables in input layer\n",
    "architecture[0] = architecture[0].format(nPinnInputs, batchsize)\n",
    "reduceLrCB = KC.ReduceLROnPlateau(monitor='loss', min_delta=0.01, patience=patience, min_lr=lrmin)\n",
    "\n",
    "assert nSample == 1, 'Can only use 1 sample with PINN architecture for now'\n",
    "assert eqId in [0, 1, 2], 'Invalid equation ID'\n",
    "assert archId in [1, 2], 'Invalid architecture ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sb9VYpLmyDBV"
   },
   "source": [
    "The next code block covers parameters needed only in the soft BC model. The number of points is set to a 10:1 ratio (collocation to boundary). To see how different ratios affect model accuracy, it is a good idea to try different variations here. For example, setting `nBndPts=10` should yield less accurate results than when training with `nBndPts=1000`. Give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B30IicJbvi3u",
    "outputId": "baa6e7cc-1115-446b-dc72-440cad3437a1"
   },
   "outputs": [],
   "source": [
    "exactBc    = False\n",
    "nColPts    = 10000\n",
    "nBndPts    = 4 * 250\n",
    "nBcPts     = 0\n",
    "nEpochSoft = 80\n",
    "nameSoft   = getFileName('model', prefix, eqId, archId, exactBc)\n",
    "\n",
    "# Traing and validation split\n",
    "nAllPts    = nColPts + nBndPts\n",
    "nTrainSoft = int(nAllPts * 0.9)\n",
    "nValidSoft = nAllPts - nTrainSoft\n",
    "print(f'{nTrainSoft} in training, {nValidSoft} in validation')\n",
    "\n",
    "# Create soft BC model\n",
    "psnNetSoft = PsnPinn(operators=architecture, reg=reg, alpha=alpha)\n",
    "psnNetSoft.compile(optimizer=KO.Adam(learning_rate=lr0), run_eagerly=eagerExec)\n",
    "\n",
    "# Callbacks\n",
    "checkpointCB = KC.ModelCheckpoint(filepath='./' + nameSoft + '/checkpoint', monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1)\n",
    "csvLogCB     = tf.keras.callbacks.CSVLogger(nameSoft + '.log', append=True)\n",
    "psnCBs       = [checkpointCB, reduceLrCB, csvLogCB]\n",
    "\n",
    "bcAF, p  = getBcAFP(nSample, fname, shape, eqId)\n",
    "trainGen = generatePtsCont(0, nTrainSoft, bcAF, p, batchsize=batchsize, nColPts=nColPts, nBndPts=nBndPts, nBcPts=nBcPts, exactBc=exactBc)\n",
    "validGen = generatePtsCont(nTrainSoft, nValidSoft, bcAF, p, batchsize=batchsize, nColPts=nColPts, nBndPts=nBndPts, nBcPts=nBcPts, exactBc=exactBc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7RsY1JLRY2P"
   },
   "source": [
    "### \"make clean\"\n",
    "\n",
    "In case you want to remove a soft BC model and its logs, run the box from below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiwCN1FSRY2Q"
   },
   "outputs": [],
   "source": [
    "#%%script false --no-raise-error\n",
    "!rm -rf $nameSoft*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2DiGXZkYY1l"
   },
   "source": [
    "### Training\n",
    "\n",
    "For illustration purposes, we will just train for a couple of epochs. At this point, the model will not be at its highest accuracy. However, the general idea and trends should be visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12sP5ZMPoMPV",
    "outputId": "83c367d2-8a18-4130-a8d9-0eacf0e399b2"
   },
   "outputs": [],
   "source": [
    "startTrain = time.perf_counter()\n",
    "\n",
    "psnNetSoft.fit(\n",
    "  trainGen,\n",
    "  batch_size=batchsize,\n",
    "  initial_epoch=initEpoch,\n",
    "  epochs=nEpochSoft,\n",
    "  steps_per_epoch=nTrainSoft//batchsize,\n",
    "  callbacks=psnCBs,\n",
    "  validation_data=validGen,\n",
    "  validation_steps=nValidSoft//batchsize,\n",
    "  verbose=True)\n",
    "\n",
    "endTrain = time.perf_counter()\n",
    "softBcTime = endTrain - startTrain\n",
    "print(f'fit() execution time in secs: {softBcTime}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__PiBsi2jTpc"
   },
   "source": [
    "### More training\n",
    "\n",
    "Optionally, we can train for a couple more epochs (and with a smaller learning rate). While not necessarily required here, we also restore the model weights which have been stored as checkpoints on disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8dUQIH5gNDi"
   },
   "outputs": [],
   "source": [
    "trainMoreSoft = True\n",
    "if trainMoreSoft:\n",
    "  assert os.path.exists(nameSoft)\n",
    "  psnNetSoft.load_weights(tf.train.latest_checkpoint(nameSoft))\n",
    "  KB.set_value(psnNetSoft.optimizer.learning_rate, lrRestart)\n",
    "\n",
    "  prevEpoch = nEpochSoft\n",
    "  nEpochSoft += 20 # train for a couple more epochs\n",
    "  startTrain = time.perf_counter()\n",
    "  psnNetSoft.fit(trainGen, batch_size=batchsize, initial_epoch=prevEpoch, \\\n",
    "                 epochs=nEpochSoft, steps_per_epoch=nTrainSoft//batchsize, callbacks=psnCBs, \\\n",
    "                 validation_data=validGen, validation_steps=nValidSoft//batchsize, verbose=True)\n",
    "  softBcTime += (time.perf_counter() - startTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzCeINbXYeJZ"
   },
   "source": [
    "### Predictions\n",
    "\n",
    "The model for soft BCs has been trained. Now, it is time to use the discrete point generator. Note that predictions are returned in a single-column vector. To plot them, they need to be reshaped to the shape of the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNnaxr0FYzfB"
   },
   "outputs": [],
   "source": [
    "predGen = generatePtsDisc(0, nGridPts, bcAF, p, batchsize=1024, shuffle=False, loop=False, exactBc=False)\n",
    "assert os.path.exists(nameSoft), f'Model {nameSoft} does not exist'\n",
    "\n",
    "phat = psnNetSoft.predict(predGen)\n",
    "phat = phat.reshape((shape[0], shape[1]))\n",
    "phat = phat[np.newaxis, :, :, np.newaxis]\n",
    "\n",
    "startPred, nPred = 0, 1\n",
    "plotSolution(startPred, nPred, p, phat, prefix, eqId, archId, exactBc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKTucS8ax1sE"
   },
   "source": [
    "### Loss\n",
    "\n",
    "Let's find out how well our model learned BCs and the Poisson PDE by plotting the losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHE64D0CO4kR"
   },
   "outputs": [],
   "source": [
    "plotMetrics(nameSoft, 0, nEpochSoft, prefix, eqId, archId, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zAEpPE4YGY1"
   },
   "source": [
    "## PINN with Exact BCs\n",
    "\n",
    "Moving on to the exact BC model. It's an extension of the soft BC model with just a couple of extra functions to process the BC.\n",
    "\n",
    "To get hold of the BC itself, we make use of the same generators defined earlier. I.e. we let the generator objects know, that this time we also need a copy of the BC (option `exactBC=True`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geGNG48qDp2b"
   },
   "source": [
    "### Model setup\n",
    "\n",
    "The exact BC model is best understood through the code below. A few things to note though:\n",
    "- `call()` uses its parent's feed-forward implementation.\n",
    "  - At the end of this call, the exact BC is \"injected\" into the result tensor.\n",
    "- `_getG()` implements Inverse-Distance-Weighting (IDW) interpolation\n",
    "  - For every point in a batch, it computes the interpolated solution value based on the distance to the BC and values at those BC points:\n",
    "  \\begin{align}\n",
    "    G(\\boldsymbol{x}) &=\n",
    "    \\sum_{i}^{N_{bc}}\n",
    "      \\frac\n",
    "      {\n",
    "        w_{i}z_{i}\n",
    "      }\n",
    "      {\n",
    "        \\sum_{i}^{N_{bc}}\n",
    "        {\n",
    "          w_{i}\n",
    "        }\n",
    "     }\\\\\n",
    "    w_i &= \\lvert\\boldsymbol{x}-\\boldsymbol{x_i}\\rvert\n",
    "  \\end{align}\n",
    "- `_getPhi()` returns a filter function\n",
    "  - It is used to reduce the network's contribution near domain boundaries where true values are enforced.\n",
    "  \\begin{align}\n",
    "    \\phi(x,y) = x \\cdot (1-x) \\cdot y \\cdot (1-y)\n",
    "  \\end{align}\n",
    "  - When training domains that are non-square or only use BCs on some of the domain sides, this function should be adjusted. Otherwise, network effects could filtered in areas where this is not desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LnqnbPgYIxp"
   },
   "outputs": [],
   "source": [
    "class PsnPinnExactBc(PsnPinn):\n",
    "  def __init__(self, **kwargs):\n",
    "    super(PsnPinnExactBc, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "  def call(self, inputs, training=False, withInputLayer=1):\n",
    "    resultTensor = super().call(inputs, training, withInputLayer)\n",
    "    xy   = inputs[0]\n",
    "    p    = inputs[1]\n",
    "    xyBc = inputs[2]\n",
    "    pBc  = inputs[3]\n",
    "    phi = self._getPhi(xy)\n",
    "    g   = self._getG(xy, p, xyBc, pBc)\n",
    "    return g + resultTensor * phi\n",
    "\n",
    "\n",
    "  def _getG(self, xy, p, xyBc, pBc, eps=1e-10, exp=2):\n",
    "    xyExp = tf.expand_dims(xy, axis=1)       # [nBatch, 1, nDim]\n",
    "    dist = tf.square(xyExp - xyBc)           # [nBatch, nXyBc, nDim]\n",
    "    dist = tf.reduce_sum(dist, axis=-1)      # [nBatch, nXyBc]\n",
    "    dist = tf.math.sqrt(dist)                # [nBatch, nXyBc]\n",
    "    wi = tf.pow(1.0 / (dist + eps), exp)     # [nBatch, nXyBc]\n",
    "    wi = tf.expand_dims(wi, axis=-1)         # [nBatch, nXyBc, 1]\n",
    "    denom = tf.reduce_sum(wi, axis=1)        # [nBatch, 1, nDim]\n",
    "    numer = tf.reduce_sum(wi * pBc, axis=1)  # [nBatch, 1, dimBc]\n",
    "    g = numer / denom                        # [nBatch, dimBc]\n",
    "    return g\n",
    "\n",
    "\n",
    "  def _getPhi(self, xy):\n",
    "    x, y  = xy[:,0], xy[:,1]\n",
    "    xMin, yMin, xMax, yMax = 0, 0, 1, 1 # Same bounds as used in domain setup\n",
    "    phi = x * (1-x) * y * (1-y)\n",
    "    phi = tf.expand_dims(phi, axis=-1)\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQewKllxhYrN"
   },
   "source": [
    "### Parameters\n",
    "\n",
    "The exact BC models use slightly different numbers of points. For instance,\n",
    "- no additional points along the domain boundary will be needed (`nBndPts=0`). This has obviously to do with the fact that the model will always return the exact BC at domain boundaries. BCs are hardcoded into `call()`.\n",
    "- only a couple of BC points will be needed for the BC vector (`nBcPts=20` for a 128x128 domain is sufficient). If the overall size of the domain increases, this number should increase too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bt_SHMJAhT3q"
   },
   "outputs": [],
   "source": [
    "exactBc     = True\n",
    "nColPts     = 10000\n",
    "nBndPts     = 0\n",
    "nBcPts      = 20\n",
    "nEpochExact = 80\n",
    "nameExact   = getFileName('model', prefix, eqId, archId, exactBc)\n",
    "\n",
    "# Traing and validation split\n",
    "nAllPts     = nColPts + nBndPts*4 # 4 because 4 domain sides\n",
    "nTrainExact = int(nAllPts * 0.9)\n",
    "nValidExact = nAllPts - nTrainExact\n",
    "print(f'{nTrainExact} in training, {nValidExact} in validation')\n",
    "\n",
    "# Create exact BC model\n",
    "psnNetExact = PsnPinnExactBc(operators=architecture, reg=reg, alpha=alpha)\n",
    "psnNetExact.compile(optimizer=KO.Adam(learning_rate=lr0), run_eagerly=eagerExec)\n",
    "\n",
    "# Callbacks\n",
    "checkpointCB = KC.ModelCheckpoint(filepath='./' + nameExact + '/checkpoint', monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1)\n",
    "csvLogCB     = KC.CSVLogger(nameExact + '.log', append=True)\n",
    "psnCBs       = [checkpointCB, reduceLrCB, csvLogCB]\n",
    "\n",
    "bcAF, p  = getBcAFP(nSample, fname, shape, eqId)\n",
    "trainGen = generatePtsCont(0, nTrainExact, bcAF, p, batchsize=batchsize, nColPts=nColPts, nBndPts=nBndPts, nBcPts=nBcPts, exactBc=exactBc)\n",
    "validGen = generatePtsCont(nTrainExact, nValidExact, bcAF, p, batchsize=batchsize, nColPts=nColPts, nBndPts=nBndPts, nBcPts=nBcPts, exactBc=exactBc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOPTcCiGQMiI"
   },
   "source": [
    "### \"make clean\"\n",
    "\n",
    "You can remove/delete the exact BC model by executing the box below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aovI4Ld6QnIE"
   },
   "outputs": [],
   "source": [
    "#%%script false --no-raise-error\n",
    "!rm -rf $nameExact*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9M-uk0bLkmVx"
   },
   "source": [
    "### Training\n",
    "\n",
    "Similarly to the soft BC model, we will train for a couple of epochs only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6kBb2TQhpFk"
   },
   "outputs": [],
   "source": [
    "startTrain = time.perf_counter()\n",
    "\n",
    "psnNetExact.fit(\n",
    "    trainGen,\n",
    "    batch_size=batchsize,\n",
    "    initial_epoch=initEpoch,\n",
    "    epochs=nEpochExact,\n",
    "    steps_per_epoch=nTrainExact//batchsize,\n",
    "    callbacks=psnCBs,\n",
    "    validation_data=validGen,\n",
    "    validation_steps=nValidExact//batchsize,\n",
    "    verbose=True)\n",
    "\n",
    "endTrain = time.perf_counter()\n",
    "exactBcTime = endTrain - startTrain\n",
    "print(f'fit() execution time in secs: {exactBcTime}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGZ8EkoIm4Ys"
   },
   "source": [
    "### More training\n",
    "\n",
    "Optionally, reduce the learning rate, load the model, and train for more epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Afn5A8Rm4Yv"
   },
   "outputs": [],
   "source": [
    "trainMoreExact = True\n",
    "if trainMoreExact:\n",
    "  assert os.path.exists(nameExact)\n",
    "  psnNetExact.load_weights(tf.train.latest_checkpoint(nameExact))\n",
    "  KB.set_value(psnNetExact.optimizer.learning_rate, lrRestart)\n",
    "\n",
    "  prevEpoch = nEpochExact\n",
    "  nEpochExact += 20 # train for a couple more epochs\n",
    "  startTrain = time.perf_counter()\n",
    "  psnNetExact.fit(trainGen, batch_size=batchsize, initial_epoch=prevEpoch, \\\n",
    "                  epochs=nEpochExact, steps_per_epoch=nTrainExact//batchsize, callbacks=psnCBs, \\\n",
    "                  validation_data=validGen, validation_steps=nValidExact//batchsize, verbose=True)\n",
    "  exactBcTime += (time.perf_counter() - startTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0I_F72and8w"
   },
   "source": [
    "### Predictions\n",
    "\n",
    "When making predictions with the exact BC model, it is important to remember that the generator needs to supply the BC vector. The vector has to be supplied during training **and** inference. Otherwise, there would be a mismatch for the scale of solution `phat`: All weights have been found for an interpolated and filtered version of `phat`, and thus, the same interpolation and filtering steps need to be followed during inference too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrtw3vT7ngxH"
   },
   "outputs": [],
   "source": [
    "predGen  = generatePtsDisc(0, nGridPts, bcAF, p, batchsize=1024, shuffle=False, loop=False, exactBc=True)\n",
    "assert os.path.exists(nameExact), f'Model {nameExact} does not exist'\n",
    "\n",
    "phatExact = psnNetExact.predict(predGen)\n",
    "phatExact = phatExact.reshape((shape[0], shape[1]))\n",
    "phatExact = phatExact[np.newaxis, :, :, np.newaxis]\n",
    "\n",
    "startPred, nPred = 0, 1\n",
    "plotSolution(startPred, nPred, p, phatExact, prefix, eqId, archId, bcId=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XitsraRrfWZH"
   },
   "source": [
    "Note how the exact BC model yields a slightly smaller error. The accuracy was improved just by \"showing\" training data in a different manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHiEQWNvNcAP"
   },
   "outputs": [],
   "source": [
    "plotMetrics(nameExact, 0, nEpochExact, prefix, eqId, archId, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MwvjDC_FVA3"
   },
   "source": [
    "## Comparing Soft and Exact BCs\n",
    "\n",
    "Depending on the dataset and the total number of epochs used during training, exact BC models generally outperform soft BC models in terms of accuracy. Given that exact BC models require fewer points to learn BCs, the time overhead caused by BC enforcement can be amortized. In some cases, exact BC models can even train faster than soft BC models (although again, this is highly dependent on the number of points in the BC vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VoWDCNvMaBWn"
   },
   "outputs": [],
   "source": [
    "table = plotMetricsTable(p=p, phatSoft=phat, phatExact=phatExact, sTime=softBcTime, eTime=exactBcTime)\n",
    "display_markdown(table, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4guNVqigq7S"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "While exact BC models have a slight edge over soft BC models, the optimal BC imposition approach will depend on the problem itself (i.e. complexity of PDE, positions where BC is enforced, number of collocation points needed for target model accuracy, etc.).\n",
    "\n",
    "These examples should give you some inspiration for your future PINN implementations. Should you decide to implement one of the strategies yourself, I'd be happy to hear about your thoughts and findings. Feel free to reach out [@sebbas](https://twitter.com/sebbas)!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
